# ğŸ“Š Model Architecture Comparison

So sÃ¡nh giá»¯a model gá»‘c vÃ  model tá»‘i Æ°u cho RTX 3050.

## ğŸ—ï¸ Architecture Comparison

### Audio Branch (Conformer Encoder)

| Component | Original | RTX 3050 Optimized | Reduction |
|-----------|----------|-------------------|-----------|
| Feature Dim | 512 | **128** | **75% â†“** |
| Num Layers | 8 | **4** | **50% â†“** |
| Attention Heads | 8 | 4 | 50% â†“ |
| FFN Dim | 2048 | 512 | 75% â†“ |
| **Params** | **~50M** | **~9M** | **82% â†“** |

### Visual Branch

| Component | Original | RTX 3050 Optimized | Reduction |
|-----------|----------|-------------------|-----------|
| Feature Dim | 512 | **128** | **75% â†“** |
| Temporal Depth | 4 | **2** | **50% â†“** |
| GSCB Layers | 2 | 1 | 50% â†“ |
| Attention Layers | 2 | 1 | 50% â†“ |
| **Params** | **~14M** | **~1M** | **93% â†“** |

### LFM2 Fusion

| Component | Original | RTX 3050 Optimized | Reduction |
|-----------|----------|-------------------|-----------|
| Audio Input | 512 | **128** | 75% â†“ |
| Visual Input | 512 | **128** | 75% â†“ |
| Hidden Dim | 1024 | **256** | **75% â†“** |
| Num Layers | 4 | **2** | **50% â†“** |
| Output Dim | 512 | **128** | **75% â†“** |
| **Params** | **~84M** | **~4M** | **95% â†“** |

### Classifier Head

| Component | Original | RTX 3050 Optimized | Reduction |
|-----------|----------|-------------------|-----------|
| Input Dim | 512 | **128** | 75% â†“ |
| Hidden Layers | [512, 256] | **[128, 64]** | 75% â†“ |
| **Params** | **~0.4M** | **~0.03M** | **92% â†“** |

## ğŸ“ˆ Overall Model Statistics

| Metric | Original | RTX 3050 Optimized | Reduction |
|--------|----------|-------------------|-----------|
| **Total Parameters** | **149M** | **~14M** | **90% â†“** |
| **FP32 Memory** | 0.60 GB | **0.06 GB** | **90% â†“** |
| **FP16 Memory** | 0.30 GB | **0.03 GB** | **90% â†“** |
| **Training VRAM** | ~3.8 GB | **~2.5 GB** | **34% â†“** |

## ğŸ¯ Training Configuration

### Dataset

| Setting | Original | RTX 3050 Optimized | Change |
|---------|----------|-------------------|--------|
| Training Samples | 1920 | **960** | **50% â†“** |
| Val Samples | 480 | 480 | Same |
| Test Samples | 480 | 480 | Same |
| Batch Size | 8-16 | **2** | **75-87% â†“** |

### Training Hyperparameters

| Setting | Original | RTX 3050 Optimized | Change |
|---------|----------|-------------------|--------|
| Epochs | 100 | **50** | 50% â†“ |
| Learning Rate | 1e-4 | 1e-4 | Same |
| Mixed Precision | FP16 | FP16 | Same |
| Gradient Clip | 1.0 | 1.0 | Same |

## â±ï¸ Time Estimation

### Per Epoch Timing

```
Training samples: 960
Batch size: 2
Batches per epoch: 480

Estimated time per batch: ~3-4 seconds
Estimated time per epoch: 480 Ã— 3.5s = 1680s â‰ˆ 28 minutes
```

### Total Training Time

```
Total epochs: 50
Time per epoch: ~28 minutes

Total training time: 50 Ã— 28 = 1400 minutes â‰ˆ 23 hours
```

**âš ï¸ LÆ°u Ã½:** ÄÃ¢y lÃ  Æ°á»›c tÃ­nh conservative. Thá»±c táº¿ cÃ³ thá»ƒ nhanh hÆ¡n:
- Epoch Ä‘áº§u tiÃªn cháº­m hÆ¡n (loading, compilation)
- CÃ¡c epoch sau nhanh hÆ¡n (~20-25 phÃºt/epoch)
- **Æ¯á»›c tÃ­nh thá»±c táº¿: 18-20 giá»**

### Breakdown by Phase

| Phase | Time per Epoch | Total Time (50 epochs) |
|-------|---------------|----------------------|
| Data Loading | ~2 min | ~1.7 hours |
| Forward Pass | ~15 min | ~12.5 hours |
| Backward Pass | ~8 min | ~6.7 hours |
| Validation | ~3 min | ~2.5 hours |
| **Total** | **~28 min** | **~23 hours** |

## ğŸ¯ Expected Performance

### Accuracy Comparison

| Metric | Original (Full) | RTX 3050 (Half) | Difference |
|--------|----------------|-----------------|------------|
| **UAR** | 0.65-0.75 | **0.50-0.60** | -0.10-0.15 |
| **Accuracy** | 0.70-0.80 | **0.55-0.65** | -0.10-0.15 |
| **WAR** | 0.68-0.78 | **0.53-0.63** | -0.10-0.15 |

**LÃ½ do performance tháº¥p hÆ¡n:**
1. âœ‚ï¸ Model nhá» hÆ¡n 90% (14M vs 149M params)
2. ğŸ“Š Chá»‰ dÃ¹ng 50% training data (960 vs 1920 samples)
3. ğŸ”¢ Batch size nhá» (2 vs 8-16) - áº£nh hÆ°á»Ÿng batch normalization
4. â±ï¸ Ãt epochs hÆ¡n (50 vs 100)

## ğŸ’¡ Trade-offs

### Advantages âœ…

1. **Fits in 4GB VRAM** - Cháº¯c cháº¯n khÃ´ng OOM
2. **Faster per epoch** - Ãt computation hÆ¡n
3. **Less overfitting risk** - Model nhá» hÆ¡n
4. **Can train locally** - KhÃ´ng cáº§n Colab

### Disadvantages âŒ

1. **Lower accuracy** - ~10-15% UAR drop
2. **Longer total time** - 18-20 giá» vs 3-4 giá» (T4)
3. **Less capacity** - KhÃ³ há»c patterns phá»©c táº¡p
4. **Smaller batch** - Training kÃ©m stable hÆ¡n

## ğŸ”„ Optimization Strategy

### Nhá»¯ng gÃ¬ Ä‘Ã£ giáº£m (theo thá»© tá»± Æ°u tiÃªn):

1. **Feature dimensions** (512â†’128): Giáº£m 75%
   - áº¢nh hÆ°á»Ÿng lá»›n nháº¥t Ä‘áº¿n params
   - Trade-off: Máº¥t capacity

2. **Number of layers** (8â†’4, 4â†’2): Giáº£m 50%
   - Giáº£m depth, giá»¯ width
   - Trade-off: Máº¥t kháº£ nÄƒng há»c hierarchical features

3. **Hidden dimensions** (1024â†’256): Giáº£m 75%
   - Trong fusion layers
   - Trade-off: Bottleneck trong fusion

4. **Training data** (1920â†’960): Giáº£m 50%
   - Nhanh hÆ¡n, Ã­t overfitting
   - Trade-off: Ãt data Ä‘á»ƒ há»c

5. **Batch size** (8â†’2): Giáº£m 75%
   - Cáº§n thiáº¿t cho VRAM
   - Trade-off: Noisy gradients

### Nhá»¯ng gÃ¬ giá»¯ nguyÃªn:

âœ… **Architecture design** - Váº«n giá»¯ cáº¥u trÃºc multimodal  
âœ… **Attention mechanisms** - Váº«n cÃ³ self-attention  
âœ… **Fusion strategy** - Váº«n dÃ¹ng LFM2  
âœ… **Learning rate** - KhÃ´ng thay Ä‘á»•i  
âœ… **Validation/Test sets** - Full data Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘Ãºng  

## ğŸ“Š Memory Breakdown (During Training)

### Original Model
```
Model weights:        0.60 GB (FP32) / 0.30 GB (FP16)
Activations:          1.50 GB
Gradients:            0.60 GB
Optimizer states:     1.20 GB
Batch data:           0.50 GB
Total:                ~3.8 GB
```

### RTX 3050 Optimized
```
Model weights:        0.06 GB (FP32) / 0.03 GB (FP16)
Activations:          0.80 GB (smaller batch + model)
Gradients:            0.06 GB
Optimizer states:     0.12 GB
Batch data:           0.25 GB (batch_size=2)
Buffer:               1.24 GB (safety margin)
Total:                ~2.5 GB (safe for 4GB)
```

## ğŸ“ Recommendations

### Náº¿u báº¡n cÃ³ thá»i gian (18-20 giá»):
âœ… **Cháº¡y script nÃ y** - An toÃ n, cháº¯c cháº¯n hoÃ n thÃ nh

### Náº¿u báº¡n cáº§n káº¿t quáº£ nhanh hÆ¡n:
1. **Giáº£m epochs xuá»‘ng 30** - Tiáº¿t kiá»‡m 40% thá»i gian
2. **DÃ¹ng Colab T4** - Nhanh hÆ¡n 5-6x
3. **Train overnight** - Äá»ƒ mÃ¡y cháº¡y qua Ä‘Ãªm

### Náº¿u báº¡n cáº§n accuracy cao hÆ¡n:
1. **TÄƒng feature dim lÃªn 192** - Compromise giá»¯a size vÃ  accuracy
2. **DÃ¹ng full dataset** - Bá» random 50%
3. **Train longer** - 100 epochs thay vÃ¬ 50

## ğŸš€ Quick Commands

### Start training (18-20 hours)
```bash
python scripts/train_half_dataset.py
```

### Monitor progress
```bash
# In another terminal
watch -n 5 nvidia-smi
```

### Check results
```bash
cat checkpoints/half_dataset_rtx3050/history.json
```

### Resume if interrupted
```bash
python scripts/resume_training.py checkpoints/half_dataset_rtx3050/checkpoint_epoch_20.pt
```

---

**Bottom line:** Model Ä‘Ã£ giáº£m 90% params nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c cáº¥u trÃºc multimodal. Thá»i gian train ~18-20 giá», accuracy dá»± kiáº¿n 0.50-0.60 UAR (acceptable cho demo/testing).
