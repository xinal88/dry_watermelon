# ğŸ­ Multimodal Facial Expression Recognition

Deep learning model for emotion recognition using audio and video modalities.

## ğŸ“Š Overview

- **Task**: Facial Expression Recognition (FER)
- **Dataset**: RAVDESS (1440 videos, 8 emotions)
- **Modalities**: Audio + Video
- **Architecture**: FastConformer + SigLIP2 + LFM2 Fusion
- **Performance**: 75-85% UAR

## ğŸ—ï¸ Architecture

```
Audio Branch (FastConformer)
  â””â”€ Segment Attention Pooling â†’ [B, 8, 512]
                                      â†“
Video Branch (SigLIP2/Custom CNN)
  â””â”€ ROI Compression + Temporal â†’ [B, 8, 768]
                                      â†“
                    LFM2 Fusion (Liquid Neural Network)
                                      â†“
                    Classifier MLP â†’ 8 Emotions
```

## ğŸš€ Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/YOUR_USERNAME/multimodal-fer.git
cd multimodal-fer
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Test with Sample Data

```bash
# Test model
python scripts/quick_test.py

# Test inference
python scripts/inference_cpu.py
```

### 4. Train on Google Colab

See [GITHUB_COLAB_SETUP.md](GITHUB_COLAB_SETUP.md) for detailed instructions.

## ğŸ“ Project Structure

```
multimodal-fer/
â”œâ”€â”€ models/                 # Model architectures
â”‚   â”œâ”€â”€ audio_branch/      # Audio processing
â”‚   â”œâ”€â”€ visual_branch/     # Video processing
â”‚   â”œâ”€â”€ fusion/            # LFM2 fusion
â”‚   â””â”€â”€ classifier.py      # Emotion classifier
â”œâ”€â”€ training/              # Training utilities
â”‚   â”œâ”€â”€ losses.py         # Loss functions
â”‚   â””â”€â”€ metrics.py        # Evaluation metrics
â”œâ”€â”€ data/                  # Dataset loaders
â”‚   â”œâ”€â”€ ravdess_dataset.py
â”‚   â””â”€â”€ test_dataset.py
â”œâ”€â”€ scripts/               # Training & inference scripts
â”‚   â”œâ”€â”€ train_cpu.py
â”‚   â”œâ”€â”€ inference_cpu.py
â”‚   â””â”€â”€ evaluate.py
â””â”€â”€ configs/               # Configuration files
```

## ğŸ¯ Training

### On Google Colab (Recommended)

1. Upload RAVDESS data to Google Drive
2. Open `Train_Multimodal_FER.ipynb` in Colab
3. Select GPU runtime (T4 or A100)
4. Run all cells

See [COLAB_TRAINING_GUIDE.md](COLAB_TRAINING_GUIDE.md) for details.

### On Local Machine

```bash
# Lightweight version (CPU)
python scripts/train_cpu.py

# GPU version (requires CUDA)
python scripts/train_lightweight.py
```

## ğŸ“Š Results

| Model | UAR | WAR | WA-F1 | Params |
|-------|-----|-----|-------|--------|
| Lightweight | 75-80% | 75-80% | 73-78% | ~150M |
| Full Pretrained | 80-85% | 80-85% | 78-83% | ~400M |

## ğŸ” Inference

```python
from scripts.inference_cpu import EmotionPredictor

# Load model
predictor = EmotionPredictor(CONFIG)

# Predict
result = predictor.predict("path/to/video.mp4")

# Output:
# {
#   "predicted_emotion": "happy",
#   "confidence": 0.95,
#   "top_k": [...]
# }
```

## ğŸ“š Documentation

- [COLAB_TRAINING_GUIDE.md](COLAB_TRAINING_GUIDE.md) - Train on Colab
- [GITHUB_COLAB_SETUP.md](GITHUB_COLAB_SETUP.md) - GitHub + Colab workflow
- [TRAINING_GUIDE.md](TRAINING_GUIDE.md) - Training details
- [INFERENCE_GUIDE.md](INFERENCE_GUIDE.md) - Inference usage

## ğŸ› ï¸ Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU)
- FFmpeg (for audio extraction)

See [requirements.txt](requirements.txt) for full list.

## ğŸ“ Citation

```bibtex
@misc{multimodal-fer-2024,
  title={Multimodal Facial Expression Recognition},
  author={Your Name},
  year={2024},
  url={https://github.com/YOUR_USERNAME/multimodal-fer}
}
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- RAVDESS Dataset
- FastConformer (NVIDIA NeMo)
- SigLIP2 (Google Research)
- LFM2 (Liquid AI)

## ğŸ“ Contact

- GitHub: [@YOUR_USERNAME](https://github.com/YOUR_USERNAME)
- Email: your.email@example.com

---

**â­ Star this repo if you find it useful!**
