# Training Configuration

training:
  # Basic settings
  max_epochs: 100
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  
  # Optimizer
  optimizer:
    name: "AdamW"
    lr: 1e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    
  # Learning rate scheduler
  scheduler:
    name: "CosineAnnealingWarmRestarts"
    T_0: 10
    T_mult: 2
    eta_min: 1e-6
    warmup_epochs: 5
    
  # Loss function
  loss:
    name: "CrossEntropyLoss"
    label_smoothing: 0.1
    class_weights: null  # or list of weights for imbalanced classes
    
  # Regularization
  regularization:
    dropout: 0.1
    mixup_alpha: 0.2  # Mixup augmentation
    cutmix_alpha: 0.0  # CutMix augmentation
    
# Validation
validation:
  check_val_every_n_epoch: 1
  val_metric: "accuracy"  # Metric to monitor for checkpointing
  
# Callbacks
callbacks:
  # Model checkpoint
  checkpoint:
    monitor: "val_accuracy"
    mode: "max"
    save_top_k: 3
    save_last: true
    dirpath: "checkpoints"
    filename: "fer-{epoch:02d}-{val_accuracy:.4f}"
    
  # Early stopping
  early_stopping:
    monitor: "val_accuracy"
    patience: 15
    mode: "max"
    min_delta: 0.001
    
  # Learning rate monitor
  lr_monitor:
    logging_interval: "step"
    
# Logging
logging:
  logger: "tensorboard"  # or "wandb"
  log_dir: "logs"
  log_every_n_steps: 10
  
  # Weights & Biases (if using wandb)
  wandb:
    project: "multimodal-fer"
    entity: null  # your wandb username
    name: null  # run name, auto-generated if null
    
# Hardware
hardware:
  accelerator: "gpu"  # or "cpu"
  devices: 1
  precision: "16-mixed"  # or "32", "bf16-mixed"
  
# Reproducibility
seed: 42
deterministic: false  # Set true for reproducibility (slower)

