# üîß Training Fixes Applied

C√°c fix ƒë√£ √°p d·ª•ng ƒë·ªÉ gi·∫£i quy·∫øt NaN loss v√† memory errors.

## ‚ùå Problems Encountered

### 1. NaN Loss During Training
```
Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 480/480 [09:00<00:00, 1.13s/it, loss=nan]
```

**Nguy√™n nh√¢n:**
- Learning rate qu√° cao (1e-4) cho model nh·ªè
- Batch size qu√° nh·ªè (2) ‚Üí noisy gradients
- Kh√¥ng c√≥ warmup ‚Üí training unstable
- Gradient explosion

### 2. Memory Error During Validation
```
SystemError: Unable to allocate 2.64 MiB for array
```

**Nguy√™n nh√¢n:**
- `num_workers > 0` t·∫°o nhi·ªÅu processes
- M·ªói worker load video v√†o RAM
- RAM kh√¥ng ƒë·ªß cho multiprocessing

## ‚úÖ Solutions Applied

### Fix 1: Reduce Learning Rate
```python
# Before
"lr": 1e-4

# After
"lr": 5e-5  # 50% reduction
```

**L√Ω do:** Model nh·ªè h∆°n c·∫ßn LR nh·ªè h∆°n ƒë·ªÉ tr√°nh divergence.

### Fix 2: Add Warmup
```python
def lr_lambda(epoch):
    warmup_epochs = 5
    if epoch < warmup_epochs:
        return (epoch + 1) / warmup_epochs
    return 1.0

warmup_scheduler = LambdaLR(optimizer, lr_lambda)
```

**L√Ω do:** Warmup gi√∫p training stable h∆°n ·ªü ƒë·∫ßu.

### Fix 3: Aggressive Gradient Clipping
```python
# Before
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

# After
torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
```

**L√Ω do:** Clip m·∫°nh h∆°n ƒë·ªÉ tr√°nh gradient explosion.

### Fix 4: NaN Detection & Skip
```python
# Check for NaN
if torch.isnan(loss) or torch.isinf(loss):
    print(f"\nWarning: NaN/Inf loss detected, skipping batch")
    continue

# Check gradient norm
grad_norm = ...
if grad_norm > 100:
    print(f"\nWarning: Large gradient norm, skipping")
    continue
```

**L√Ω do:** Skip bad batches thay v√¨ crash.

### Fix 5: Disable Multiprocessing
```python
# Before
"num_workers": 2

# After
"num_workers": 0  # Single process
```

**L√Ω do:** Tr√°nh memory errors t·ª´ multiprocessing.

### Fix 6: Add Numerical Stability
```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=CONFIG["lr"],
    weight_decay=CONFIG["weight_decay"],
    eps=1e-8,  # Add epsilon
)
```

**L√Ω do:** Tr√°nh division by zero trong optimizer.

## üìä Expected Behavior Now

### Training
```
Epoch 1/50
----------------------------------------------------------------------
Training: 100%|‚ñà‚ñà‚ñà‚ñà| 480/480 [10:00<00:00, loss=2.0543, grad=0.85]
Validation: 100%|‚ñà‚ñà| 240/240 [02:30<00:00]

Results:
  Train Loss: 2.0543
  Val Loss:   1.8234
  Accuracy:   0.3542
  UAR:        0.3125
  Time:       750.3s
```

### Loss Progression
```
Epoch 1:  loss=2.05 (high, normal)
Epoch 5:  loss=1.65 (decreasing)
Epoch 10: loss=1.35 (stable)
Epoch 20: loss=1.15 (converging)
Epoch 50: loss=0.95 (final)
```

## ‚ö†Ô∏è Trade-offs

### Slower Training
- `num_workers=0` ‚Üí Ch·∫≠m h∆°n ~20%
- M·ªói epoch: ~30 ph√∫t ‚Üí ~36 ph√∫t
- Total: 18-20 gi·ªù ‚Üí **22-24 gi·ªù**

### More Stable
- ‚úÖ Kh√¥ng b·ªã NaN loss
- ‚úÖ Kh√¥ng b·ªã memory errors
- ‚úÖ Training smooth h∆°n
- ‚úÖ Ch·∫Øc ch·∫Øn ho√†n th√†nh

## üéØ New Time Estimate

```
Per epoch: ~36 minutes (was 28 minutes)
50 epochs: 36 √ó 50 = 1800 minutes = 30 hours

Realistic: 22-24 hours (epochs sau nhanh h∆°n)
```

## üí° If Still Having Issues

### If NaN persists:
```python
# Reduce LR further
"lr": 2e-5  # Even lower

# Or increase warmup
warmup_epochs = 10  # More warmup
```

### If memory errors persist:
```python
# Reduce batch size
"batch_size": 1  # Extreme case

# Or reduce video resolution in ravdess_dataset.py
video_size: (112, 112)  # Half resolution
```

### If too slow:
```python
# Reduce epochs
"num_epochs": 30  # 60% of time

# Or reduce dataset further
half_size = train_size // 4  # Use 25% instead of 50%
```

## üöÄ Ready to Train

Script ƒë√£ ƒë∆∞·ª£c fix v√† s·∫µn s√†ng:

```bash
python scripts/train_half_dataset.py
```

Expected:
- ‚úÖ No NaN loss
- ‚úÖ No memory errors  
- ‚úÖ Stable training
- ‚è±Ô∏è 22-24 hours total
- üéØ UAR 0.50-0.60

---

**Note:** Training s·∫Ω ch·∫≠m h∆°n d·ª± ki·∫øn ban ƒë·∫ßu (22-24h thay v√¨ 18-20h) nh∆∞ng ch·∫Øc ch·∫Øn ho√†n th√†nh v√† stable.
